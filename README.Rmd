---
title: "Tf-idf analysis of the economic report of the president"
author: "Mitsuo Shiota"
date: "2020/2/27"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
Updated: `r Sys.Date()`

## Motivation

I have found ["Text Mining Fedspeak" by Len Kiefer](http://lenkiefer.com/2018/07/28/text-mining-fedspeak/), which applies  tidytext techniques to analyze the annual Federal Reserve Monetary Policy Report, very interesting. Especially the td-idf analysis part is fascinating, as it reveals what the Fed is talking abount most by year. For example, they talked a lot about "iraq", "war" and "sars" in 2003, and "subprime" in 2007 and 2008. I could feel the history.

So I use the same techniques to analyze the economic report of the president from 1947 to 2021.

## Libraries

As usual, I attach tidyverse package. I also attach tidytext package, which provides text mining techniques.

```{r libraries, include=FALSE}
library(tidyverse)
library(tidytext)

```

## Preparation
### Download and split

I download the economic report of the president from 1947 to 2019 from [FRASER  by Federal Reserve Bank of St. Louis](https://fraser.stlouisfed.org/title/45), and from 2020 to 2021 from [govinfo.gov](https://www.govinfo.gov/app/collection/erp/).

I first tried to download each pdf into a temporary file, and to extract text by pdftools::pdf_text(). But pdftools::pdf_text() hangs, when it tries the 2009 report. When I make the 2009 report smaller by excluding appendixes, pdftools::pdf_text works. So I changed my strategy. I download all pdf files in the directory data/before_split, split them, and save the files, which exclude appendixes, in the directory data/after_split.

I have already downloaded the pdf files by download.py whose codes are below. I appreciate the digitization works done by the Federal Reserve Bank of St. Louis. My small complaint is URL inconsistency. Now before_split directory size is around 530MB.

```{python download, echo=FALSE, eval=FALSE, python.reticulate=FALSE}
import requests
import time

for year in range(1947, 2021):
    if year <= 1949:
        # Although there are midyear reports from 1949 to 1952, I ignore them
        url = 'https://fraser.stlouisfed.org/files/docs/publications/ERP/{0}/ERP_{0}_January.pdf'.format(
            year)
    elif year <= 1952:
        url = 'https://fraser.stlouisfed.org/files/docs/publications/ERP/{0}/ERP_January_{0}.pdf'.format(
            year)
    elif year <= 1986:
        url = 'https://fraser.stlouisfed.org/files/docs/publications/ERP/{0}/ERP_{0}.pdf'.format(
            year)
    elif year <= 1988:
        url = 'https://fraser.stlouisfed.org/files/docs/publications/ERP/{0}/ER_{0}.pdf'.format(
            year)
    elif year <= 2008:
        url = 'https://fraser.stlouisfed.org/files/docs/publications/ERP/{0}/ERP_{0}.pdf'.format(
            year)
    elif year == 2009:
        url = 'https://fraser.stlouisfed.org/files/docs/publications/ERP/{0}/{0}_ERP.pdf'.format(
            year)
    elif year == 2010:
        url = 'https://fraser.stlouisfed.org/files/docs/publications/ERP/{0}/erp_{0}.pdf'.format(
            year)
    elif year <= 2014:
        url = 'https://fraser.stlouisfed.org/files/docs/publications/ERP/{0}/{0}_erp.pdf'.format(
            year)
    elif year <= 2019:
        url = 'https://fraser.stlouisfed.org/files/docs/publications/ERP/{}_erp.pdf'.format(
            year)
    else:
        url = 'https://www.whitehouse.gov/wp-content/uploads/2020/02/2020-Economic-Report-of-the-President-WHCEA.pdf'

    output_filename = 'data/before_split/{}_erp.pdf'.format(year)

    myfile = requests.get(url)

    with open(output_filename, 'wb') as out:
        out.write(myfile.content)

    print(year)
    time.sleep(20)

```
Next I manually check which page starts Appendix A, split pdf files, and save the files which exclude appendixes in the directory data/after_split. I have used split.py whose codes are below. Now after_split directory size is around 460MB.

```{python split, echo=FALSE, eval=FALSE, python.reticulate=FALSE}
from PyPDF2 import PdfFileReader, PdfFileWriter


def pdf_split(year, app_A_start_page):

    input_filename = 'data/before_split/{}_erp.pdf'.format(year)

    pdf = PdfFileReader(input_filename)

    pdf_writer = PdfFileWriter()

    for page in range(app_A_start_page-1):
        pdf_writer.addPage(pdf.getPage(page))

    output_filename = 'data/after_split/{}_excl_app.pdf'.format(year)

    with open(output_filename, 'wb') as out:
        pdf_writer.write(out)


year = range(1947, 2022)
app_A_start_page = [44, 98, 110,
                    134, 176, 160, 160, 126, 79, 109, 85, 85, 78,
                    84, 82, 199, 160, 173, 177, 193, 204, 201, 218,
                    148, 171, 182, 147, 233, 224, 159, 176, 244, 169,
                    190, 220, 222, 152, 208, 223, 241, 233, 236, 297,
                    271, 270, 285, 331, 255, 251, 265, 286, 267, 312,
                    278, 245, 295, 251, 270, 186, 252, 203, 202, 261,
                    310, 170, 291, 303, 350, 370, 386, 549, 514, 619,
                    348, 440]

for year, app_A_start_page in zip(year, app_A_start_page):
    pdf_split(year, app_A_start_page)

```
### Extract text

I adapt pdftools::pdf_text() to each file in the directory data/after_split. pdf_text() returns a text by page in a list. After I have done all files, I get the list by report, each of which is the list by page.

```{r read, include=FALSE}
files <- dir("data/after_split", pattern = "\\.pdf$", full.names = TRUE)

erp_list <- vector("list", length = length(files))

for (i in seq_along(erp_list)) {
  erp_list[[i]] <- pdftools::pdf_text(files[i])
  print(files[i])
}

names(erp_list) <- str_sub(files, 18L, 30L)

```
There may be some pdf problems in the 2005 report. I check like below, and it looks OK.

```{r check 2005}
length(erp_list$`2005_excl_app`)
str(erp_list$`2005_excl_app`)

```
As the Federal Reserve Bank of St. Louis added some signatures to its digitization work, I remove them.

```{r remove digitized, echo=FALSE}
remove_digitized <- function(report) {
  map(report,
      ~ str_remove(.x, "Digitized for FRASER") %>% 
        str_remove("http://fraser.stlouisfed.org/") %>% 
        str_remove("Federal Reserve Bank of St. Louis"))
}

erp_list <- map(erp_list, remove_digitized)

```
I save the list of lists in rdata format. The data size is 15MB.

```{r save_rdata}
save(erp_list, file = "data/erp.rdata")

```

### Transform a list of lists into a dataframe

First I transform a list of lists first into a list of dataframes whose columns are "text", "report" and "page." Next I split text by line, and add a column, "line." When I finish every list and bind rows, I get a dataframe. I replace "report" column with "year" column.

```{r transform_into_dataframe, echo=FALSE}
erp_text_raw <- vector("list", length = length(erp_list))

for (i in seq_along(erp_text_raw)) {
  erp_text_raw[[i]] <- 
    tibble(
      text = unlist(erp_list[[i]]),
      report = names(erp_list)[i],
      page = 1:length(erp_list[[i]])
    ) %>% 
    mutate(text = str_split(text,"\r")) %>% 
    unnest(text) %>% 
    group_by(report, page) %>% 
    mutate(line = row_number()) %>% 
    ungroup() %>% 
    mutate(text = str_remove_all(text, "\n"))
  
}

erp_text_raw_df <- bind_rows(erp_text_raw)

erp_text_raw_df <- erp_text_raw_df %>% 
  mutate(year = as.integer(str_sub(report, 1L, 4L))) %>% 
  select(-report)

```

## How many pages in a report?

The number of pages increased significantly in recent years.

```{r plot_total_pages, echo=FALSE}
erp_text_raw_df %>% 
  group_by(year) %>% 
  summarize(n_pages = max(page)) %>% 
  ggplot(aes(year, n_pages)) +
  geom_line() +
  labs(
    title = "President and CEA reports, excluding Appendices",
    x = "Year",
    y = "# of pages"
  )

```

I check manually which pages are the president part, and not the CEA (Council of Economic Advisers) part, in each report, and build data.frame. In 1947-48 and 1954-61, the reports of  are not clearly separated between the president part, "The Economic Report of the President", and the CEA part, "The Annual Report of the Council of Economic Advisers", so I distinguish by my judegement. Basically I specify the page of "To the Congress of the United States:" as the start page of the president part.

The number of pages in the president part has been less than 10 since 1982, which was the first one signed by Reagan.

```{r plot_pres_pages, echo=FALSE}
pages_pres <- tibble(
  year = 1947:2021,
  start = c(10, 9, 9,
            9, 9, 9, 9, 4, 5, 5, 5, 5, 5,
            5, 5, 11, 11, 11, 9, 9, 9, 9, 9,
            9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
            9, 9, 9, 9, 7, 9, 9, 9, 9, 9,
            9, 9, 9, 9, 9, 4, 7, 9, 9, 9,
            4, 4, 4, 5, 8, 5, 5, 5, 5, 4,
            8, 8, 7, 7, 8, 8, 8, 8, 8, 8,
            8, 8),
  end = c(11, 18, 26,
          25, 33, 39, 35, 6, 8, 8, 9, 8, 9,
          7, 7, 35, 30, 26, 27, 27, 32, 34, 30,
          17, 15, 13, 13, 16, 14, 14, 17, 29, 21,
          21, 25, 16, 14, 13, 15, 17, 14, 16, 17,
          14, 16, 11, 11, 14, 9, 10, 11, 11, 12,
          8, 6, 5, 6, 9, 7, 7, 7, 7, 8,
          14, 12, 9, 9, 11, 11, 11, 11, 16, 11,
          13, 12)
)

pages_pres <- pages_pres %>% 
  mutate(n_pages = end - start + 1)

pages_pres %>% 
  ggplot(aes(year, n_pages)) +
  geom_line() +
  labs(
    title = "President reports, excluding CEA reports and Appendices",
    x = "Year",
    y = "# of pages"
  )

```

## Tokenize and count most frequently used words

Now I am ready to utilize tidytext functions. I tokenize texts into words, remove all words which contain non-alphabetical characters

```{r tokenize}
erp_text <- erp_text_raw_df %>% 
  unnest_tokens(word, text) %>% 
  mutate(word = str_remove_all(word, "[^A-Za-z ]")) %>%
  filter(word != "")

erp_text

```

I count words. The result is uninteresting.

```{r count_words}
erp_text  %>%
  count(word, sort = TRUE) 

```

Next I exclude stop words which are "a", "the", and something like that. The result is still boring, as I know the reports are about the economy.

```{r count_without_stopwords}
erp_text  %>%
  anti_join(stop_words, by = "word")%>%
  count(word, sort = TRUE) 

```

## tf-idf (term frequency–inverse document frequency) analysis

So I turn to tf-idf analysis, which scores high the words which appear frequently in this year's report, but seldom appear in the other years' reports.

The highest score naturally goes to "covid" in 2021. The high score of "https" is due to richer references in recent reports.

```{r tf-idf}
erp_textb <- erp_text %>% 
  count(year, word, sort = TRUE) %>% 
  bind_tf_idf(word, year, n) %>%
  arrange(desc(tf_idf))

erp_textb

```

## Top 10 words by year

Next I would like to show top 10 words by year. I tried several times, and somewhat arbitrarily picked custom stop words. I prepare the plot function for drawing.

```{r plot_function}
custom_stop_words <- 
  bind_rows(tibble(word = c("gdp",
                            "gnp",
                            "box",
                            "cea",
                            "ceas",
                            "yr",
                            "fy",
                            "pdf", "http", "https"),
                   lexicon = c("custom")), 
            stop_words)

plot_tf_idf <- function(year_start, year_end) {
  erp_textb %>% 
    filter(year >= year_start, year <= year_end) %>% 
    anti_join(custom_stop_words, by="word") %>%
    mutate(word = reorder_within(word, tf_idf, year)) %>% 
    group_by(year) %>% 
    top_n(10, tf_idf) %>% 
    ungroup() %>% 
    ggplot(aes(word, tf_idf, fill = year)) +
    geom_col(show.legend = FALSE) +
    labs(x = NULL, y = "tf-idf") +
    facet_wrap(~ year, scales = "free", ncol = 5) +
    coord_flip() +
    scale_x_reordered()
}

```

Digitization is not perfect. When I search "inn" in 1956, I find some axis parts of charts as "inn". So, you may see some strange words below. Also, be aware that the scale of the tf-idf axis is not the same across years.

Anyway, can't you feel the history?

I found "yen" and "japanese" in 1984, read the report, and found the report complained a lot about the undervalued Japanese yen. The report seems to have been the prelude to the Plaza Accord of 1985. That is a history, or is it?

```{r year_1947_1961, fig.width=10, fig.height=8}
plot_tf_idf(1947, 1961)

```

```{r year_1962_1976, fig.width=10, fig.height=8}
plot_tf_idf(1962, 1976)

```

```{r year_1977_1991, fig.width=10, fig.height=8}
plot_tf_idf(1977, 1991)

```

```{r year_1992_2006, fig.width=10, fig.height=8}
plot_tf_idf(1992, 2006)

```

```{r year_2007_2021, fig.width=10, fig.height=8}
plot_tf_idf(2007, 2021)

```

EOL
