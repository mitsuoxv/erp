{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbVYHv45Vx2y"
   },
   "source": [
    "# Let the presidents speak on economic matters\n",
    "\n",
    "This is basically a copy of [NLP Zero to Hero](https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%204%20-%20Lesson%202%20-%20Notebook.ipynb). The only change is that the inputs are not Irish lyrics, but the United States Presidents' writings in the Economic Report of the President from 1947 to 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0AZ87T9ZCGg"
   },
   "source": [
    "## Preprocess\n",
    "\n",
    "I started from the pdf files I downloaded for the [tf-idf](https://github.com/mitsuoxv/erp/blob/master/README.md) and [sentiment analysis](https://github.com/mitsuoxv/erp/blob/master/Sentiment.md) in this repository. First, I made new pdf files which contain only the president parts by make_pres_pdf.py. Next, I extract texts into new files by make_pres_txt.py, which utilizes data_func.py I copied from [PDF Text Extraction in Python](https://towardsdatascience.com/pdf-text-extraction-in-python-5b6ab9e92dd).\n",
    "\n",
    "Then I copied text files into 'texts/presidents/' directory, and manually cleaned up texts by:\n",
    "\n",
    "1. correcting word order, where digitization made mistakes in shaping lines;\n",
    "\n",
    "1. correcting words, where optical recognition made mistakes due to dirt, or the author apparently misspelled;\n",
    "\n",
    "1. making punctuation common over reports, following my memory of [\"The Mac is Not a Typewriter\" by Robin P. Williams](https://www.goodreads.com/book/show/41600.The_Mac_is_Not_a_Typewriter), which I must have, but could not find now. As an exception, using minus mark instead of hyphen; and\n",
    "\n",
    "1. changing lines, where I encounter \";\", \":\", \"—\" or \",\" and when I feel changed lines are natural if they were in President Johnson's reports.\n",
    "\n",
    "I tried to be consistent over reports of different authors. Honestly, I am not sure whether \"text\" has become more consistent or not, as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5aX7BoXX_Zg"
   },
   "source": [
    "I download the texts from 'texts/presidents/' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HUq8hmX8-1bN",
    "outputId": "9f8cee52-4bc6-4854-88d4-b99845047d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1947_pres.txt  1962_pres.txt  1977_pres.txt  1992_pres.txt  2007_pres.txt\r\n",
      "1948_pres.txt  1963_pres.txt  1978_pres.txt  1993_pres.txt  2008_pres.txt\r\n",
      "1949_pres.txt  1964_pres.txt  1979_pres.txt  1994_pres.txt  2009_pres.txt\r\n",
      "1950_pres.txt  1965_pres.txt  1980_pres.txt  1995_pres.txt  2010_pres.txt\r\n",
      "1951_pres.txt  1966_pres.txt  1981_pres.txt  1996_pres.txt  2011_pres.txt\r\n",
      "1952_pres.txt  1967_pres.txt  1982_pres.txt  1997_pres.txt  2012_pres.txt\r\n",
      "1953_pres.txt  1968_pres.txt  1983_pres.txt  1998_pres.txt  2013_pres.txt\r\n",
      "1954_pres.txt  1969_pres.txt  1984_pres.txt  1999_pres.txt  2014_pres.txt\r\n",
      "1955_pres.txt  1970_pres.txt  1985_pres.txt  2000_pres.txt  2015_pres.txt\r\n",
      "1956_pres.txt  1971_pres.txt  1986_pres.txt  2001_pres.txt  2016_pres.txt\r\n",
      "1957_pres.txt  1972_pres.txt  1987_pres.txt  2002_pres.txt  2017_pres.txt\r\n",
      "1958_pres.txt  1973_pres.txt  1988_pres.txt  2003_pres.txt  2018_pres.txt\r\n",
      "1959_pres.txt  1974_pres.txt  1989_pres.txt  2004_pres.txt  2019_pres.txt\r\n",
      "1960_pres.txt  1975_pres.txt  1990_pres.txt  2005_pres.txt  2020_pres.txt\r\n",
      "1961_pres.txt  1976_pres.txt  1991_pres.txt  2006_pres.txt\r\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/mitsuoxv/erp/master/texts/presidents/{1947..2020}_pres.txt --directory-prefix=texts/presidents/\n",
    "!ls texts/presidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbUSwRW1oLTb"
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for year in range(1947, 2020):\n",
    "    input_filename = 'texts/presidents/{}_pres.txt'.format(year)\n",
    "    \n",
    "    with open(input_filename, 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    corpus = corpus + text.lower().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3ttxeZyEic2"
   },
   "source": [
    "I remove bullet only lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zldAC4C3BKJz"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "corpus = [re.sub('^.$', '', line) for line in corpus]\n",
    "corpus = [re.sub('^\\\\(.\\\\)$', '', line) for line in corpus]\n",
    "corpus = [re.sub('^.\\\\.$', '', line) for line in corpus]\n",
    "\n",
    "corpus = [line for line in corpus if line != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41oijgMYedR3"
   },
   "source": [
    "So far the number of lines are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TMhaYE-VoLTh",
    "outputId": "0ea8d8d2-87f1-4c4e-a631-fbc2e5c31e7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12990"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QyrgmcbaJePm"
   },
   "source": [
    "I remove the lines of which number of words is equal to, or greater than 35, as I know those can be regarded as exceptions from [Sentiment analysis using R](https://github.com/mitsuoxv/erp/blob/master/Sentiment.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "miVj3_WAHIgn"
   },
   "outputs": [],
   "source": [
    "corpus = [line for line in corpus if len(re.split('[ —]', line)) < 35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8r7pz-DKZz3"
   },
   "source": [
    "The number of lines are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S-T_WP1ZJX79",
    "outputId": "cce54f02-ef24-4653-ab4a-4736d9b8c3e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12077"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-yXxBWQSoLTl",
    "outputId": "06970067-9495-44f6-cddb-ebfe738660c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0-rc1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to [Stackoverflow](https://stackoverflow.com/questions/56333388/tensorflow-fail-to-find-dnn-implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKt7NwyOoLTu"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jCx6UiDoLTw"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_c7F6HPuoLTz",
    "outputId": "b9477e88-6849-4217-8348-81acfb917723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9176\n"
     ]
    }
   ],
   "source": [
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sk9Tw0sNoLT1"
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HFdB_x7oLT3"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(20)))\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RXcbSM2joLT6"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "Bc01WSrQoLT_",
    "outputId": "3bb017d5-2a6f-4db0-b02e-5e2340458cba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 37, 64)            587264    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 40)                13600     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 9176)              376216    \n",
      "=================================================================\n",
      "Total params: 977,080\n",
      "Trainable params: 977,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8dilZG1gjqx"
   },
   "source": [
    "As model.fit is expected to take aproximately 33 hours, it stopped at 186th epock. Probably Colab does not allow this long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PZJfAaXKoLT8",
    "outputId": "9e452d18-b52f-484c-e403-db0c849b198d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "6111/6111 [==============================] - 64s 11ms/step - loss: 6.5122 - accuracy: 0.0889\n",
      "Epoch 2/500\n",
      "6111/6111 [==============================] - 64s 10ms/step - loss: 5.9061 - accuracy: 0.1379\n",
      "Epoch 3/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 5.5967 - accuracy: 0.1607\n",
      "Epoch 4/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 5.3942 - accuracy: 0.1751\n",
      "Epoch 5/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 5.2404 - accuracy: 0.1862\n",
      "Epoch 6/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 5.1196 - accuracy: 0.1948\n",
      "Epoch 7/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 5.0183 - accuracy: 0.2020\n",
      "Epoch 8/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.9248 - accuracy: 0.2095\n",
      "Epoch 9/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.8340 - accuracy: 0.2161\n",
      "Epoch 10/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.7519 - accuracy: 0.2221\n",
      "Epoch 11/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.6806 - accuracy: 0.2276\n",
      "Epoch 12/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.6163 - accuracy: 0.2323\n",
      "Epoch 13/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.5568 - accuracy: 0.2369\n",
      "Epoch 14/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.4988 - accuracy: 0.2415\n",
      "Epoch 15/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.4436 - accuracy: 0.2447\n",
      "Epoch 16/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.3929 - accuracy: 0.2484\n",
      "Epoch 17/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.3458 - accuracy: 0.2520\n",
      "Epoch 18/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.3028 - accuracy: 0.2550\n",
      "Epoch 19/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 4.2608 - accuracy: 0.2588\n",
      "Epoch 20/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 4.2226 - accuracy: 0.2620\n",
      "Epoch 21/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.1844 - accuracy: 0.2646\n",
      "Epoch 22/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 4.1479 - accuracy: 0.2679\n",
      "Epoch 23/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.1114 - accuracy: 0.2703\n",
      "Epoch 24/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 4.0786 - accuracy: 0.2734\n",
      "Epoch 25/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.0449 - accuracy: 0.2762\n",
      "Epoch 26/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 4.0127 - accuracy: 0.2794\n",
      "Epoch 27/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.9818 - accuracy: 0.2821\n",
      "Epoch 28/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.9507 - accuracy: 0.2845\n",
      "Epoch 29/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.9228 - accuracy: 0.2869\n",
      "Epoch 30/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.8959 - accuracy: 0.2898\n",
      "Epoch 31/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.8686 - accuracy: 0.2916\n",
      "Epoch 32/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.8444 - accuracy: 0.2942\n",
      "Epoch 33/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.8205 - accuracy: 0.2965\n",
      "Epoch 34/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.7979 - accuracy: 0.2991\n",
      "Epoch 35/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.7774 - accuracy: 0.3015\n",
      "Epoch 36/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.7562 - accuracy: 0.3036\n",
      "Epoch 37/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.7366 - accuracy: 0.3053\n",
      "Epoch 38/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.7167 - accuracy: 0.3077\n",
      "Epoch 39/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.6997 - accuracy: 0.3096\n",
      "Epoch 40/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.6824 - accuracy: 0.3106\n",
      "Epoch 41/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.6674 - accuracy: 0.3131\n",
      "Epoch 42/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.6505 - accuracy: 0.3151\n",
      "Epoch 43/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.6372 - accuracy: 0.3160\n",
      "Epoch 44/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.6209 - accuracy: 0.3183\n",
      "Epoch 45/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.6076 - accuracy: 0.3197\n",
      "Epoch 46/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.5959 - accuracy: 0.3215\n",
      "Epoch 47/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.5831 - accuracy: 0.3228\n",
      "Epoch 48/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.5683 - accuracy: 0.3242\n",
      "Epoch 49/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.5580 - accuracy: 0.3261\n",
      "Epoch 50/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.5463 - accuracy: 0.3272\n",
      "Epoch 51/500\n",
      "6111/6111 [==============================] - 65s 11ms/step - loss: 3.5324 - accuracy: 0.3281\n",
      "Epoch 52/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.5216 - accuracy: 0.3294\n",
      "Epoch 53/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.5096 - accuracy: 0.3313\n",
      "Epoch 54/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.4997 - accuracy: 0.3328\n",
      "Epoch 55/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.4904 - accuracy: 0.3322\n",
      "Epoch 56/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.4810 - accuracy: 0.3343\n",
      "Epoch 57/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.4707 - accuracy: 0.3372\n",
      "Epoch 58/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.4588 - accuracy: 0.3377\n",
      "Epoch 59/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.4542 - accuracy: 0.3381\n",
      "Epoch 60/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.4436 - accuracy: 0.3407\n",
      "Epoch 61/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.4320 - accuracy: 0.3406\n",
      "Epoch 62/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.4265 - accuracy: 0.3413\n",
      "Epoch 63/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.4163 - accuracy: 0.3431\n",
      "Epoch 64/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.4089 - accuracy: 0.3435\n",
      "Epoch 65/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.4002 - accuracy: 0.3452\n",
      "Epoch 66/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3943 - accuracy: 0.3452\n",
      "Epoch 67/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3863 - accuracy: 0.3465\n",
      "Epoch 68/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3783 - accuracy: 0.3483\n",
      "Epoch 69/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3717 - accuracy: 0.3484\n",
      "Epoch 70/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3647 - accuracy: 0.3502\n",
      "Epoch 71/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3588 - accuracy: 0.3500\n",
      "Epoch 72/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3523 - accuracy: 0.3517\n",
      "Epoch 73/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3452 - accuracy: 0.3521\n",
      "Epoch 74/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3375 - accuracy: 0.3535\n",
      "Epoch 75/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3323 - accuracy: 0.3538\n",
      "Epoch 76/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3302 - accuracy: 0.3533\n",
      "Epoch 77/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3186 - accuracy: 0.3566\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3139 - accuracy: 0.3573\n",
      "Epoch 79/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3110 - accuracy: 0.3561\n",
      "Epoch 80/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.3045 - accuracy: 0.3576\n",
      "Epoch 81/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2978 - accuracy: 0.3582\n",
      "Epoch 82/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2941 - accuracy: 0.3591\n",
      "Epoch 83/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2902 - accuracy: 0.3591\n",
      "Epoch 84/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2852 - accuracy: 0.3604\n",
      "Epoch 85/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2797 - accuracy: 0.3603\n",
      "Epoch 86/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2738 - accuracy: 0.3610\n",
      "Epoch 87/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2692 - accuracy: 0.3622\n",
      "Epoch 88/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2668 - accuracy: 0.3624\n",
      "Epoch 89/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2599 - accuracy: 0.3638\n",
      "Epoch 90/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2559 - accuracy: 0.3632\n",
      "Epoch 91/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2551 - accuracy: 0.3648\n",
      "Epoch 92/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2465 - accuracy: 0.3654\n",
      "Epoch 93/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2449 - accuracy: 0.3650\n",
      "Epoch 94/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2391 - accuracy: 0.3663\n",
      "Epoch 95/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2349 - accuracy: 0.3659\n",
      "Epoch 96/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2356 - accuracy: 0.3662\n",
      "Epoch 97/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2279 - accuracy: 0.3679\n",
      "Epoch 98/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2224 - accuracy: 0.3684\n",
      "Epoch 99/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2189 - accuracy: 0.3691\n",
      "Epoch 100/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2200 - accuracy: 0.3695\n",
      "Epoch 101/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2105 - accuracy: 0.3701\n",
      "Epoch 102/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2117 - accuracy: 0.3700\n",
      "Epoch 103/500\n",
      "6111/6111 [==============================] - 67s 11ms/step - loss: 3.2023 - accuracy: 0.3723\n",
      "Epoch 104/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.2027 - accuracy: 0.3709\n",
      "Epoch 105/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1983 - accuracy: 0.3723\n",
      "Epoch 106/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1965 - accuracy: 0.3717\n",
      "Epoch 107/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1953 - accuracy: 0.3728\n",
      "Epoch 108/500\n",
      "6111/6111 [==============================] - 67s 11ms/step - loss: 3.1902 - accuracy: 0.3731\n",
      "Epoch 109/500\n",
      "6111/6111 [==============================] - 67s 11ms/step - loss: 3.1842 - accuracy: 0.3739\n",
      "Epoch 110/500\n",
      "6111/6111 [==============================] - 67s 11ms/step - loss: 3.1833 - accuracy: 0.3741\n",
      "Epoch 111/500\n",
      "6111/6111 [==============================] - 67s 11ms/step - loss: 3.1783 - accuracy: 0.3736\n",
      "Epoch 112/500\n",
      "6111/6111 [==============================] - 67s 11ms/step - loss: 3.1754 - accuracy: 0.3745\n",
      "Epoch 113/500\n",
      "6111/6111 [==============================] - 67s 11ms/step - loss: 3.1723 - accuracy: 0.3754\n",
      "Epoch 114/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1714 - accuracy: 0.3754\n",
      "Epoch 115/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1659 - accuracy: 0.3765\n",
      "Epoch 116/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1671 - accuracy: 0.3762\n",
      "Epoch 117/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1649 - accuracy: 0.3758\n",
      "Epoch 118/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1577 - accuracy: 0.3771\n",
      "Epoch 119/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1568 - accuracy: 0.3775\n",
      "Epoch 120/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1586 - accuracy: 0.3774\n",
      "Epoch 121/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1559 - accuracy: 0.3779\n",
      "Epoch 122/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1509 - accuracy: 0.3783\n",
      "Epoch 123/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1491 - accuracy: 0.3786\n",
      "Epoch 124/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1438 - accuracy: 0.3792\n",
      "Epoch 125/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1427 - accuracy: 0.3797\n",
      "Epoch 126/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1455 - accuracy: 0.3794\n",
      "Epoch 127/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1398 - accuracy: 0.3795\n",
      "Epoch 128/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1413 - accuracy: 0.3791\n",
      "Epoch 129/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1314 - accuracy: 0.3811\n",
      "Epoch 130/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1337 - accuracy: 0.3802\n",
      "Epoch 131/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1315 - accuracy: 0.3802\n",
      "Epoch 132/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1243 - accuracy: 0.3812\n",
      "Epoch 133/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1261 - accuracy: 0.3817\n",
      "Epoch 134/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1247 - accuracy: 0.3822\n",
      "Epoch 135/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1182 - accuracy: 0.3822\n",
      "Epoch 136/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1179 - accuracy: 0.3825\n",
      "Epoch 137/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1205 - accuracy: 0.3815\n",
      "Epoch 138/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1154 - accuracy: 0.3836\n",
      "Epoch 139/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1165 - accuracy: 0.3837\n",
      "Epoch 140/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1105 - accuracy: 0.3840\n",
      "Epoch 141/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1067 - accuracy: 0.3839\n",
      "Epoch 142/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1146 - accuracy: 0.3823\n",
      "Epoch 143/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1041 - accuracy: 0.3846\n",
      "Epoch 144/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1056 - accuracy: 0.3841\n",
      "Epoch 145/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1021 - accuracy: 0.3848\n",
      "Epoch 146/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1005 - accuracy: 0.3844\n",
      "Epoch 147/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.1028 - accuracy: 0.3849\n",
      "Epoch 148/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0938 - accuracy: 0.3857\n",
      "Epoch 149/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0957 - accuracy: 0.3853\n",
      "Epoch 150/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0919 - accuracy: 0.3863\n",
      "Epoch 151/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0897 - accuracy: 0.3865\n",
      "Epoch 152/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0878 - accuracy: 0.3870\n",
      "Epoch 153/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0926 - accuracy: 0.3866\n",
      "Epoch 154/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0835 - accuracy: 0.3876\n",
      "Epoch 155/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0863 - accuracy: 0.3869\n",
      "Epoch 156/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0853 - accuracy: 0.3871\n",
      "Epoch 157/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0778 - accuracy: 0.3878\n",
      "Epoch 158/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0815 - accuracy: 0.3874\n",
      "Epoch 159/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0816 - accuracy: 0.3879\n",
      "Epoch 160/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0775 - accuracy: 0.3878\n",
      "Epoch 161/500\n",
      "6111/6111 [==============================] - 67s 11ms/step - loss: 3.0772 - accuracy: 0.3883\n",
      "Epoch 162/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0721 - accuracy: 0.3889\n",
      "Epoch 163/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0757 - accuracy: 0.3882\n",
      "Epoch 164/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0702 - accuracy: 0.3889\n",
      "Epoch 165/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0696 - accuracy: 0.3881\n",
      "Epoch 166/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0648 - accuracy: 0.3892\n",
      "Epoch 167/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0675 - accuracy: 0.3895\n",
      "Epoch 168/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0692 - accuracy: 0.3892\n",
      "Epoch 169/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0684 - accuracy: 0.3896\n",
      "Epoch 170/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0622 - accuracy: 0.3907\n",
      "Epoch 171/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0646 - accuracy: 0.3907\n",
      "Epoch 172/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0625 - accuracy: 0.3905\n",
      "Epoch 173/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0627 - accuracy: 0.3911\n",
      "Epoch 174/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0604 - accuracy: 0.3897\n",
      "Epoch 175/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0603 - accuracy: 0.3897\n",
      "Epoch 176/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0530 - accuracy: 0.3916\n",
      "Epoch 177/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0610 - accuracy: 0.3904\n",
      "Epoch 178/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0555 - accuracy: 0.3914\n",
      "Epoch 179/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0541 - accuracy: 0.3919\n",
      "Epoch 180/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0529 - accuracy: 0.3912\n",
      "Epoch 181/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0478 - accuracy: 0.3922\n",
      "Epoch 182/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0493 - accuracy: 0.3914\n",
      "Epoch 183/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0518 - accuracy: 0.3916\n",
      "Epoch 184/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0463 - accuracy: 0.3933\n",
      "Epoch 185/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0470 - accuracy: 0.3923\n",
      "Epoch 186/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0413 - accuracy: 0.3934\n",
      "Epoch 187/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0405 - accuracy: 0.3934\n",
      "Epoch 188/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0422 - accuracy: 0.3938\n",
      "Epoch 189/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0477 - accuracy: 0.3918\n",
      "Epoch 190/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0414 - accuracy: 0.3941\n",
      "Epoch 191/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0401 - accuracy: 0.3939\n",
      "Epoch 192/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0402 - accuracy: 0.3935\n",
      "Epoch 193/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0407 - accuracy: 0.3933\n",
      "Epoch 194/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0351 - accuracy: 0.3944\n",
      "Epoch 195/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0332 - accuracy: 0.3947\n",
      "Epoch 196/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0316 - accuracy: 0.3939\n",
      "Epoch 197/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0327 - accuracy: 0.3938\n",
      "Epoch 198/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0268 - accuracy: 0.3952\n",
      "Epoch 199/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0283 - accuracy: 0.3955\n",
      "Epoch 200/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0275 - accuracy: 0.3949\n",
      "Epoch 201/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0294 - accuracy: 0.3953\n",
      "Epoch 202/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0284 - accuracy: 0.3947\n",
      "Epoch 203/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0304 - accuracy: 0.3940\n",
      "Epoch 204/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0302 - accuracy: 0.3945\n",
      "Epoch 205/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0262 - accuracy: 0.3952\n",
      "Epoch 206/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0241 - accuracy: 0.3955\n",
      "Epoch 207/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0256 - accuracy: 0.3947\n",
      "Epoch 208/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0214 - accuracy: 0.3961\n",
      "Epoch 209/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0195 - accuracy: 0.3964\n",
      "Epoch 210/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0194 - accuracy: 0.3968\n",
      "Epoch 211/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0191 - accuracy: 0.3953\n",
      "Epoch 212/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0153 - accuracy: 0.3965\n",
      "Epoch 213/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0211 - accuracy: 0.3951\n",
      "Epoch 214/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0135 - accuracy: 0.3972\n",
      "Epoch 215/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0185 - accuracy: 0.3966\n",
      "Epoch 216/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0151 - accuracy: 0.3969\n",
      "Epoch 217/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0147 - accuracy: 0.3972\n",
      "Epoch 218/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0149 - accuracy: 0.3980\n",
      "Epoch 219/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0113 - accuracy: 0.3977\n",
      "Epoch 220/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0120 - accuracy: 0.3974\n",
      "Epoch 221/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0046 - accuracy: 0.3990\n",
      "Epoch 222/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0078 - accuracy: 0.3979\n",
      "Epoch 223/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0059 - accuracy: 0.3989\n",
      "Epoch 224/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0154 - accuracy: 0.3969\n",
      "Epoch 225/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0042 - accuracy: 0.3982\n",
      "Epoch 226/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0002 - accuracy: 0.3990\n",
      "Epoch 227/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0059 - accuracy: 0.3985\n",
      "Epoch 228/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0054 - accuracy: 0.3985\n",
      "Epoch 229/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0016 - accuracy: 0.3988\n",
      "Epoch 230/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0038 - accuracy: 0.3975\n",
      "Epoch 231/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9996 - accuracy: 0.3988\n",
      "Epoch 232/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0056 - accuracy: 0.3984\n",
      "Epoch 233/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0053 - accuracy: 0.3988\n",
      "Epoch 234/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0000 - accuracy: 0.3987\n",
      "Epoch 235/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0028 - accuracy: 0.3991\n",
      "Epoch 236/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0002 - accuracy: 0.3988\n",
      "Epoch 237/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9982 - accuracy: 0.3989\n",
      "Epoch 238/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9907 - accuracy: 0.4001\n",
      "Epoch 239/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0003 - accuracy: 0.3998\n",
      "Epoch 240/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0031 - accuracy: 0.3990\n",
      "Epoch 241/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9985 - accuracy: 0.3996\n",
      "Epoch 242/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9976 - accuracy: 0.3991\n",
      "Epoch 243/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9939 - accuracy: 0.3999\n",
      "Epoch 244/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9931 - accuracy: 0.4003\n",
      "Epoch 245/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9951 - accuracy: 0.3993\n",
      "Epoch 246/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9897 - accuracy: 0.4008\n",
      "Epoch 247/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9925 - accuracy: 0.4008\n",
      "Epoch 248/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9926 - accuracy: 0.3995\n",
      "Epoch 249/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9907 - accuracy: 0.4004\n",
      "Epoch 250/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9900 - accuracy: 0.4004\n",
      "Epoch 251/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9846 - accuracy: 0.4018\n",
      "Epoch 252/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9891 - accuracy: 0.4007\n",
      "Epoch 253/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9983 - accuracy: 0.3992\n",
      "Epoch 254/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9919 - accuracy: 0.4003\n",
      "Epoch 255/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9866 - accuracy: 0.4016\n",
      "Epoch 256/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 3.0004 - accuracy: 0.3992\n",
      "Epoch 257/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9881 - accuracy: 0.4001\n",
      "Epoch 258/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9848 - accuracy: 0.4027\n",
      "Epoch 259/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9898 - accuracy: 0.4000\n",
      "Epoch 260/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9869 - accuracy: 0.4012\n",
      "Epoch 261/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9824 - accuracy: 0.4018\n",
      "Epoch 262/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9848 - accuracy: 0.4009\n",
      "Epoch 263/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9885 - accuracy: 0.4005\n",
      "Epoch 264/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9823 - accuracy: 0.4017\n",
      "Epoch 265/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9857 - accuracy: 0.4016\n",
      "Epoch 266/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9755 - accuracy: 0.4030\n",
      "Epoch 267/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9782 - accuracy: 0.4017\n",
      "Epoch 268/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9865 - accuracy: 0.4008\n",
      "Epoch 269/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9802 - accuracy: 0.4023\n",
      "Epoch 270/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9773 - accuracy: 0.4011\n",
      "Epoch 271/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9818 - accuracy: 0.4013\n",
      "Epoch 272/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9792 - accuracy: 0.4024\n",
      "Epoch 273/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9807 - accuracy: 0.4016\n",
      "Epoch 274/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9750 - accuracy: 0.4024\n",
      "Epoch 275/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9737 - accuracy: 0.4022\n",
      "Epoch 276/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9768 - accuracy: 0.4023\n",
      "Epoch 277/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9780 - accuracy: 0.4013\n",
      "Epoch 278/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9753 - accuracy: 0.4024\n",
      "Epoch 279/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9792 - accuracy: 0.4019\n",
      "Epoch 280/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9771 - accuracy: 0.4019\n",
      "Epoch 281/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9812 - accuracy: 0.4023\n",
      "Epoch 282/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9728 - accuracy: 0.4024\n",
      "Epoch 283/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9761 - accuracy: 0.4029\n",
      "Epoch 284/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9799 - accuracy: 0.4021\n",
      "Epoch 285/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9724 - accuracy: 0.4036\n",
      "Epoch 286/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9788 - accuracy: 0.4024\n",
      "Epoch 287/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9744 - accuracy: 0.4027\n",
      "Epoch 288/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9759 - accuracy: 0.4025\n",
      "Epoch 289/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9770 - accuracy: 0.4031\n",
      "Epoch 290/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9683 - accuracy: 0.4035\n",
      "Epoch 291/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9733 - accuracy: 0.4031\n",
      "Epoch 292/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9714 - accuracy: 0.4029\n",
      "Epoch 293/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9747 - accuracy: 0.4029\n",
      "Epoch 294/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9676 - accuracy: 0.4034\n",
      "Epoch 295/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9675 - accuracy: 0.4025\n",
      "Epoch 296/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9700 - accuracy: 0.4035\n",
      "Epoch 297/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9699 - accuracy: 0.4029\n",
      "Epoch 298/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9609 - accuracy: 0.4050\n",
      "Epoch 299/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9804 - accuracy: 0.4014\n",
      "Epoch 300/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9664 - accuracy: 0.4041\n",
      "Epoch 301/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9688 - accuracy: 0.4028\n",
      "Epoch 302/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9651 - accuracy: 0.4038\n",
      "Epoch 303/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9607 - accuracy: 0.4043\n",
      "Epoch 304/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9675 - accuracy: 0.4037\n",
      "Epoch 305/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9710 - accuracy: 0.4024\n",
      "Epoch 306/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9670 - accuracy: 0.4031\n",
      "Epoch 307/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9589 - accuracy: 0.4048\n",
      "Epoch 308/500\n",
      "6111/6111 [==============================] - 66s 11ms/step - loss: 2.9680 - accuracy: 0.4037\n",
      "Epoch 309/500\n",
      "1830/6111 [=======>......................] - ETA: 46s - loss: 2.8843 - accuracy: 0.4172"
     ]
    }
   ],
   "source": [
    "history = model.fit(xs, ys, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7-Y1eYFnPLwa",
    "outputId": "15832cd9-d6fc-4bd9-94ef-55d3c5275360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f2bb4a14cc0>\n"
     ]
    }
   ],
   "source": [
    "model.save('ErpPresModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7N-WAI-NPReP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "wNRXeMAjPYgD",
    "outputId": "9216e973-78f3-47dc-9fb0-1fa61636d079"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gH_NOUUvevEV"
   },
   "source": [
    "## Text Generation\n",
    "\n",
    "I put the seed text as below. The model will generate 100 words after the seed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNFUnCEGPc3c"
   },
   "outputs": [],
   "source": [
    "seed_text = \"I have some proposals to the Congress\"\n",
    "next_words = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "T2DzxoitP8hq",
    "outputId": "195e9396-30d6-465d-e840-0126a06e39bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-30-0901c9067b9b>:4: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "I have some proposals to the Congress presented 1947 that i have included in the people and the policy for a few days i have a month earlier resources in a rate of prices and wages which more permanently sustainable rates of the vast public debt and to check year improved services to sustained economic situation to deal with the situation points up generally i evaluate it should should i evaluate i deal show i evaluate wholesale control of our program among as of deflationary pressures of living through hospitals and to savings and credit wages prosperity to reduce of low incomes health or strikes soon as\n"
     ]
    }
   ],
   "source": [
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAe6drImfHJU"
   },
   "source": [
    "Looks like English. Sounds like presidential?\n",
    "\n",
    "My ambition was to create some A. I. Presidents, like a Democrat President or a Republican President, and let them talk about health care, tax, trade, etc. However, the generated texts so far are not convincing. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "let_pres_speak_failed.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
