{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbVYHv45Vx2y"
   },
   "source": [
    "# Let the presidents speak on economic matters\n",
    "\n",
    "This is basically a copy of [NLP Zero to Hero](https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%204%20-%20Lesson%202%20-%20Notebook.ipynb). The only change is that the inputs are not Irish lyrics, but the United States Presidents' writings in the Economic Report of the President from 1947 to 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0AZ87T9ZCGg"
   },
   "source": [
    "## Preprocess\n",
    "\n",
    "I started from the pdf files I downloaded for the [tf-idf](https://github.com/mitsuoxv/erp/blob/master/README.md) and [sentiment analysis](https://github.com/mitsuoxv/erp/blob/master/Sentiment.md) in this repository. First, I made new pdf files which contain only the president parts by make_pres_pdf.py. Next, I extract texts into new files by make_pres_txt.py, which utilizes data_func.py I copied from [PDF Text Extraction in Python](https://towardsdatascience.com/pdf-text-extraction-in-python-5b6ab9e92dd).\n",
    "\n",
    "Then I copied text files into 'texts/presidents/' directory, and manually cleaned up texts by:\n",
    "\n",
    "1. correcting word order, where digitization made mistakes in shaping lines;\n",
    "\n",
    "1. correcting words, where optical recognition made mistakes due to dirt, or the author apparently misspelled;\n",
    "\n",
    "1. making punctuation common over reports, following my memory of [\"The Mac is Not a Typewriter\" by Robin P. Williams](https://www.goodreads.com/book/show/41600.The_Mac_is_Not_a_Typewriter), which I must have, but could not find now. As an exception, using minus mark instead of hyphen; and\n",
    "\n",
    "1. changing lines, where I encounter \";\", \":\", \"—\" or \",\" and when I feel changed lines are natural if they were in President Johnson's reports.\n",
    "\n",
    "I tried to be consistent over reports of different authors. Honestly, I am not sure whether \"text\" has become more consistent or not, as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5aX7BoXX_Zg"
   },
   "source": [
    "I download the texts from 'texts/presidents/' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HUq8hmX8-1bN",
    "outputId": "9f8cee52-4bc6-4854-88d4-b99845047d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1947_pres.txt  1962_pres.txt  1977_pres.txt  1992_pres.txt  2007_pres.txt\r\n",
      "1948_pres.txt  1963_pres.txt  1978_pres.txt  1993_pres.txt  2008_pres.txt\r\n",
      "1949_pres.txt  1964_pres.txt  1979_pres.txt  1994_pres.txt  2009_pres.txt\r\n",
      "1950_pres.txt  1965_pres.txt  1980_pres.txt  1995_pres.txt  2010_pres.txt\r\n",
      "1951_pres.txt  1966_pres.txt  1981_pres.txt  1996_pres.txt  2011_pres.txt\r\n",
      "1952_pres.txt  1967_pres.txt  1982_pres.txt  1997_pres.txt  2012_pres.txt\r\n",
      "1953_pres.txt  1968_pres.txt  1983_pres.txt  1998_pres.txt  2013_pres.txt\r\n",
      "1954_pres.txt  1969_pres.txt  1984_pres.txt  1999_pres.txt  2014_pres.txt\r\n",
      "1955_pres.txt  1970_pres.txt  1985_pres.txt  2000_pres.txt  2015_pres.txt\r\n",
      "1956_pres.txt  1971_pres.txt  1986_pres.txt  2001_pres.txt  2016_pres.txt\r\n",
      "1957_pres.txt  1972_pres.txt  1987_pres.txt  2002_pres.txt  2017_pres.txt\r\n",
      "1958_pres.txt  1973_pres.txt  1988_pres.txt  2003_pres.txt  2018_pres.txt\r\n",
      "1959_pres.txt  1974_pres.txt  1989_pres.txt  2004_pres.txt  2019_pres.txt\r\n",
      "1960_pres.txt  1975_pres.txt  1990_pres.txt  2005_pres.txt  2020_pres.txt\r\n",
      "1961_pres.txt  1976_pres.txt  1991_pres.txt  2006_pres.txt\r\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/mitsuoxv/erp/master/texts/presidents/{1947..2020}_pres.txt --directory-prefix=texts/presidents/\n",
    "!ls texts/presidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbUSwRW1oLTb"
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for year in range(1947, 2020):\n",
    "    input_filename = 'texts/presidents/{}_pres.txt'.format(year)\n",
    "    \n",
    "    with open(input_filename, 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    corpus = corpus + text.lower().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3ttxeZyEic2"
   },
   "source": [
    "I remove bullet only lines, remove empty lines, substitute em dash by space, and remove double quotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zldAC4C3BKJz"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "corpus = [line for line in corpus if not re.fullmatch(r'^.$', line)]\n",
    "corpus = [line for line in corpus if not re.fullmatch(r'^\\(.\\)$', line)]\n",
    "corpus = [line for line in corpus if not re.fullmatch(r'^.\\.$', line)]\n",
    "\n",
    "corpus = [line for line in corpus if line != \"\"]\n",
    "\n",
    "corpus = [re.sub('—', ' ', line) for line in corpus]\n",
    "corpus = [re.sub('“', '', line) for line in corpus]\n",
    "corpus = [re.sub('”', '', line) for line in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41oijgMYedR3"
   },
   "source": [
    "So far the number of lines are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TMhaYE-VoLTh",
    "outputId": "0ea8d8d2-87f1-4c4e-a631-fbc2e5c31e7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12990"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QyrgmcbaJePm"
   },
   "source": [
    "I remove the lines of which number of words is equal to, or greater than 35, as I know those can be regarded as exceptions from [Sentiment analysis using R](https://github.com/mitsuoxv/erp/blob/master/Sentiment.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "miVj3_WAHIgn"
   },
   "outputs": [],
   "source": [
    "corpus = [line for line in corpus if len(re.split('[ —]', line)) < 35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8r7pz-DKZz3"
   },
   "source": [
    "The number of lines are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S-T_WP1ZJX79",
    "outputId": "cce54f02-ef24-4653-ab4a-4736d9b8c3e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12077"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-yXxBWQSoLTl",
    "outputId": "06970067-9495-44f6-cddb-ebfe738660c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0-rc1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I add this line to avoid dnn implementation error. I followed the advise in [this StackOverflow page](https://stackoverflow.com/questions/56333388/tensorflow-fail-to-find-dnn-implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKt7NwyOoLTu"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jCx6UiDoLTw"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_c7F6HPuoLTz",
    "outputId": "b9477e88-6849-4217-8348-81acfb917723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8449\n"
     ]
    }
   ],
   "source": [
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sk9Tw0sNoLT1"
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HFdB_x7oLT3"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(20)))\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RXcbSM2joLT6"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "Bc01WSrQoLT_",
    "outputId": "3bb017d5-2a6f-4db0-b02e-5e2340458cba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 37, 64)            540736    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 40)                13600     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8449)              346409    \n",
      "=================================================================\n",
      "Total params: 900,745\n",
      "Trainable params: 900,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8dilZG1gjqx"
   },
   "source": [
    "Beware model.fit is expected to take aproximately 8 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PZJfAaXKoLT8",
    "outputId": "9e452d18-b52f-484c-e403-db0c849b198d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "6130/6130 [==============================] - 64s 10ms/step - loss: 6.4800 - accuracy: 0.0876\n",
      "Epoch 2/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 5.9157 - accuracy: 0.1336\n",
      "Epoch 3/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 5.5985 - accuracy: 0.1567\n",
      "Epoch 4/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 5.3946 - accuracy: 0.1709\n",
      "Epoch 5/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 5.2447 - accuracy: 0.1825\n",
      "Epoch 6/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 5.1195 - accuracy: 0.1915\n",
      "Epoch 7/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 5.0025 - accuracy: 0.2003\n",
      "Epoch 8/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 4.8987 - accuracy: 0.2080\n",
      "Epoch 9/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 4.8086 - accuracy: 0.2147\n",
      "Epoch 10/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 4.7301 - accuracy: 0.2193\n",
      "Epoch 11/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 4.6592 - accuracy: 0.2254\n",
      "Epoch 12/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 4.5956 - accuracy: 0.2298\n",
      "Epoch 13/500\n",
      "6130/6130 [==============================] - 64s 11ms/step - loss: 4.5368 - accuracy: 0.2347\n",
      "Epoch 14/500\n",
      "6130/6130 [==============================] - 64s 11ms/step - loss: 4.4844 - accuracy: 0.2380\n",
      "Epoch 15/500\n",
      "6130/6130 [==============================] - 64s 11ms/step - loss: 4.4351 - accuracy: 0.2420\n",
      "Epoch 16/500\n",
      "6130/6130 [==============================] - 64s 10ms/step - loss: 4.3910 - accuracy: 0.2447\n",
      "Epoch 17/500\n",
      "6130/6130 [==============================] - 64s 10ms/step - loss: 4.3492 - accuracy: 0.2478\n",
      "Epoch 18/500\n",
      "6130/6130 [==============================] - 64s 10ms/step - loss: 4.3087 - accuracy: 0.2508\n",
      "Epoch 19/500\n",
      "6130/6130 [==============================] - 64s 11ms/step - loss: 4.2714 - accuracy: 0.2542\n",
      "Epoch 20/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 4.2339 - accuracy: 0.2575\n",
      "Epoch 21/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 4.1994 - accuracy: 0.2595\n",
      "Epoch 22/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 4.1655 - accuracy: 0.2627\n",
      "Epoch 23/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 4.1354 - accuracy: 0.2641\n",
      "Epoch 24/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 4.1069 - accuracy: 0.2673\n",
      "Epoch 25/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 4.0792 - accuracy: 0.2700\n",
      "Epoch 26/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 4.0548 - accuracy: 0.2722\n",
      "Epoch 27/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 4.0319 - accuracy: 0.2736\n",
      "Epoch 28/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 4.0070 - accuracy: 0.2762\n",
      "Epoch 29/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.9866 - accuracy: 0.2779\n",
      "Epoch 30/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 3.9660 - accuracy: 0.2796\n",
      "Epoch 31/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.9539 - accuracy: 0.2810\n",
      "Epoch 32/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.9297 - accuracy: 0.2831\n",
      "Epoch 33/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 3.9118 - accuracy: 0.2853\n",
      "Epoch 34/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.8927 - accuracy: 0.2870\n",
      "Epoch 35/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.8762 - accuracy: 0.2883\n",
      "Epoch 36/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 3.8598 - accuracy: 0.2905\n",
      "Epoch 37/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.8448 - accuracy: 0.2910\n",
      "Epoch 38/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.8286 - accuracy: 0.2930\n",
      "Epoch 39/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.8148 - accuracy: 0.2938\n",
      "Epoch 40/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 3.7995 - accuracy: 0.2954\n",
      "Epoch 41/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 3.7868 - accuracy: 0.2976\n",
      "Epoch 42/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.7724 - accuracy: 0.2988\n",
      "Epoch 43/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.7621 - accuracy: 0.2998\n",
      "Epoch 44/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 3.7495 - accuracy: 0.3011\n",
      "Epoch 45/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.7416 - accuracy: 0.3024\n",
      "Epoch 46/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.7227 - accuracy: 0.3042\n",
      "Epoch 47/500\n",
      "6130/6130 [==============================] - 65s 11ms/step - loss: 3.7142 - accuracy: 0.3054\n",
      "Epoch 48/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.7044 - accuracy: 0.3063\n",
      "Epoch 49/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.6947 - accuracy: 0.3078\n",
      "Epoch 50/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.6849 - accuracy: 0.3084\n",
      "Epoch 51/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.6760 - accuracy: 0.3097\n",
      "Epoch 52/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.6646 - accuracy: 0.3112\n",
      "Epoch 53/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.6560 - accuracy: 0.3111\n",
      "Epoch 54/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.6476 - accuracy: 0.3121\n",
      "Epoch 55/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.6393 - accuracy: 0.3128\n",
      "Epoch 56/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.6306 - accuracy: 0.3138\n",
      "Epoch 57/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.6209 - accuracy: 0.3155\n",
      "Epoch 58/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.6136 - accuracy: 0.3155\n",
      "Epoch 59/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.6049 - accuracy: 0.3171\n",
      "Epoch 60/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.6022 - accuracy: 0.3173\n",
      "Epoch 61/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5900 - accuracy: 0.3194\n",
      "Epoch 62/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5828 - accuracy: 0.3199\n",
      "Epoch 63/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5773 - accuracy: 0.3206\n",
      "Epoch 64/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5678 - accuracy: 0.3220\n",
      "Epoch 65/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5648 - accuracy: 0.3223\n",
      "Epoch 66/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5590 - accuracy: 0.3217\n",
      "Epoch 67/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5490 - accuracy: 0.3233\n",
      "Epoch 68/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5438 - accuracy: 0.3246\n",
      "Epoch 69/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5356 - accuracy: 0.3256\n",
      "Epoch 70/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5316 - accuracy: 0.3258\n",
      "Epoch 71/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5281 - accuracy: 0.3265\n",
      "Epoch 72/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5191 - accuracy: 0.3276\n",
      "Epoch 73/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5173 - accuracy: 0.3268\n",
      "Epoch 74/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5095 - accuracy: 0.3279\n",
      "Epoch 75/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5035 - accuracy: 0.3289\n",
      "Epoch 76/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.5006 - accuracy: 0.3288\n",
      "Epoch 77/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.4928 - accuracy: 0.3308\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4867 - accuracy: 0.3314\n",
      "Epoch 79/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.4820 - accuracy: 0.3322\n",
      "Epoch 80/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4784 - accuracy: 0.3327\n",
      "Epoch 81/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4730 - accuracy: 0.3327\n",
      "Epoch 82/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4677 - accuracy: 0.3340\n",
      "Epoch 83/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4614 - accuracy: 0.3343\n",
      "Epoch 84/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4560 - accuracy: 0.3358\n",
      "Epoch 85/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.4508 - accuracy: 0.3359\n",
      "Epoch 86/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.4459 - accuracy: 0.3358\n",
      "Epoch 87/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4432 - accuracy: 0.3367\n",
      "Epoch 88/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4392 - accuracy: 0.3372\n",
      "Epoch 89/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4356 - accuracy: 0.3370\n",
      "Epoch 90/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4299 - accuracy: 0.3387\n",
      "Epoch 91/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4333 - accuracy: 0.3384\n",
      "Epoch 92/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4226 - accuracy: 0.3396\n",
      "Epoch 93/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4170 - accuracy: 0.3409\n",
      "Epoch 94/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4143 - accuracy: 0.3403\n",
      "Epoch 95/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4124 - accuracy: 0.3411\n",
      "Epoch 96/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4002 - accuracy: 0.3419\n",
      "Epoch 97/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.4009 - accuracy: 0.3419\n",
      "Epoch 98/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3981 - accuracy: 0.3429\n",
      "Epoch 99/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3920 - accuracy: 0.3437\n",
      "Epoch 100/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3888 - accuracy: 0.3440\n",
      "Epoch 101/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3831 - accuracy: 0.3444\n",
      "Epoch 102/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3811 - accuracy: 0.3449\n",
      "Epoch 103/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3802 - accuracy: 0.3443\n",
      "Epoch 104/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3856 - accuracy: 0.3446\n",
      "Epoch 105/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3697 - accuracy: 0.3462\n",
      "Epoch 106/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3713 - accuracy: 0.3460\n",
      "Epoch 107/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3722 - accuracy: 0.3455\n",
      "Epoch 108/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3600 - accuracy: 0.3479\n",
      "Epoch 109/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3571 - accuracy: 0.3475\n",
      "Epoch 110/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3600 - accuracy: 0.3462\n",
      "Epoch 111/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3589 - accuracy: 0.3476\n",
      "Epoch 112/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3507 - accuracy: 0.3480\n",
      "Epoch 113/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3550 - accuracy: 0.3480\n",
      "Epoch 114/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3548 - accuracy: 0.3489\n",
      "Epoch 115/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3456 - accuracy: 0.3488\n",
      "Epoch 116/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3416 - accuracy: 0.3496\n",
      "Epoch 117/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3383 - accuracy: 0.3504\n",
      "Epoch 118/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3392 - accuracy: 0.3501\n",
      "Epoch 119/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3343 - accuracy: 0.3509\n",
      "Epoch 120/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3279 - accuracy: 0.3518\n",
      "Epoch 121/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3324 - accuracy: 0.3502\n",
      "Epoch 122/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3249 - accuracy: 0.3523\n",
      "Epoch 123/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3265 - accuracy: 0.3518\n",
      "Epoch 124/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3226 - accuracy: 0.3525\n",
      "Epoch 125/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3208 - accuracy: 0.3516\n",
      "Epoch 126/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3159 - accuracy: 0.3523\n",
      "Epoch 127/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3267 - accuracy: 0.3520\n",
      "Epoch 128/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3084 - accuracy: 0.3543\n",
      "Epoch 129/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3093 - accuracy: 0.3543\n",
      "Epoch 130/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.3068 - accuracy: 0.3539\n",
      "Epoch 131/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3076 - accuracy: 0.3540\n",
      "Epoch 132/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.3044 - accuracy: 0.3546\n",
      "Epoch 133/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2962 - accuracy: 0.3550\n",
      "Epoch 134/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2955 - accuracy: 0.3559\n",
      "Epoch 135/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2957 - accuracy: 0.3565\n",
      "Epoch 136/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2933 - accuracy: 0.3561\n",
      "Epoch 137/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2879 - accuracy: 0.3568\n",
      "Epoch 138/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2891 - accuracy: 0.3569\n",
      "Epoch 139/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2910 - accuracy: 0.3564\n",
      "Epoch 140/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2849 - accuracy: 0.3574\n",
      "Epoch 141/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2810 - accuracy: 0.3582\n",
      "Epoch 142/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2857 - accuracy: 0.3574\n",
      "Epoch 143/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2830 - accuracy: 0.3579\n",
      "Epoch 144/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2761 - accuracy: 0.3586\n",
      "Epoch 145/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2746 - accuracy: 0.3583\n",
      "Epoch 146/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2683 - accuracy: 0.3595\n",
      "Epoch 147/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2762 - accuracy: 0.3594\n",
      "Epoch 148/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2665 - accuracy: 0.3601\n",
      "Epoch 149/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2696 - accuracy: 0.3589\n",
      "Epoch 150/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2647 - accuracy: 0.3598\n",
      "Epoch 151/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2810 - accuracy: 0.3576\n",
      "Epoch 152/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2612 - accuracy: 0.3605\n",
      "Epoch 153/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2589 - accuracy: 0.3606\n",
      "Epoch 154/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2629 - accuracy: 0.3607\n",
      "Epoch 155/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2641 - accuracy: 0.3599\n",
      "Epoch 156/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2562 - accuracy: 0.3613\n",
      "Epoch 157/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2678 - accuracy: 0.3594\n",
      "Epoch 158/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2556 - accuracy: 0.3616\n",
      "Epoch 159/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2494 - accuracy: 0.3622\n",
      "Epoch 160/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2506 - accuracy: 0.3619\n",
      "Epoch 161/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2498 - accuracy: 0.3617\n",
      "Epoch 162/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2492 - accuracy: 0.3630\n",
      "Epoch 163/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2445 - accuracy: 0.3629\n",
      "Epoch 164/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2438 - accuracy: 0.3633\n",
      "Epoch 165/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2394 - accuracy: 0.3636\n",
      "Epoch 166/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2457 - accuracy: 0.3628\n",
      "Epoch 167/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2347 - accuracy: 0.3634\n",
      "Epoch 168/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2375 - accuracy: 0.3645\n",
      "Epoch 169/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2325 - accuracy: 0.3645\n",
      "Epoch 170/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2427 - accuracy: 0.3631\n",
      "Epoch 171/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2334 - accuracy: 0.3649\n",
      "Epoch 172/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2366 - accuracy: 0.3643\n",
      "Epoch 173/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2255 - accuracy: 0.3664\n",
      "Epoch 174/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2325 - accuracy: 0.3649\n",
      "Epoch 175/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2218 - accuracy: 0.3656\n",
      "Epoch 176/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2220 - accuracy: 0.3663\n",
      "Epoch 177/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2230 - accuracy: 0.3663\n",
      "Epoch 178/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2197 - accuracy: 0.3661\n",
      "Epoch 179/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2225 - accuracy: 0.3659\n",
      "Epoch 180/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2224 - accuracy: 0.3669\n",
      "Epoch 181/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2259 - accuracy: 0.3655\n",
      "Epoch 182/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2177 - accuracy: 0.3670\n",
      "Epoch 183/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2147 - accuracy: 0.3674\n",
      "Epoch 184/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2284 - accuracy: 0.3666\n",
      "Epoch 185/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2128 - accuracy: 0.3681\n",
      "Epoch 186/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2133 - accuracy: 0.3671\n",
      "Epoch 187/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2131 - accuracy: 0.3683\n",
      "Epoch 188/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2161 - accuracy: 0.3678\n",
      "Epoch 189/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2162 - accuracy: 0.3665\n",
      "Epoch 190/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2141 - accuracy: 0.3671\n",
      "Epoch 191/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2151 - accuracy: 0.3668\n",
      "Epoch 192/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2095 - accuracy: 0.3681\n",
      "Epoch 193/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2069 - accuracy: 0.3682\n",
      "Epoch 194/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2040 - accuracy: 0.3691\n",
      "Epoch 195/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2036 - accuracy: 0.3688\n",
      "Epoch 196/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.2011 - accuracy: 0.3706\n",
      "Epoch 197/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2065 - accuracy: 0.3683\n",
      "Epoch 198/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2010 - accuracy: 0.3704\n",
      "Epoch 199/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1965 - accuracy: 0.3707\n",
      "Epoch 200/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2011 - accuracy: 0.3699\n",
      "Epoch 201/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1984 - accuracy: 0.3690\n",
      "Epoch 202/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1899 - accuracy: 0.3708\n",
      "Epoch 203/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1962 - accuracy: 0.3690\n",
      "Epoch 204/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1876 - accuracy: 0.3712\n",
      "Epoch 205/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1917 - accuracy: 0.3703\n",
      "Epoch 206/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1895 - accuracy: 0.3710\n",
      "Epoch 207/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1883 - accuracy: 0.3705\n",
      "Epoch 208/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1872 - accuracy: 0.3714\n",
      "Epoch 209/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1931 - accuracy: 0.3707\n",
      "Epoch 210/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1844 - accuracy: 0.3718\n",
      "Epoch 211/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1857 - accuracy: 0.3713\n",
      "Epoch 212/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1895 - accuracy: 0.3709\n",
      "Epoch 213/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1967 - accuracy: 0.3696\n",
      "Epoch 214/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1968 - accuracy: 0.3704\n",
      "Epoch 215/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1821 - accuracy: 0.3717\n",
      "Epoch 216/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1857 - accuracy: 0.3705\n",
      "Epoch 217/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1837 - accuracy: 0.3725\n",
      "Epoch 218/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1798 - accuracy: 0.3720\n",
      "Epoch 219/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1823 - accuracy: 0.3712\n",
      "Epoch 220/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1794 - accuracy: 0.3720\n",
      "Epoch 221/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1824 - accuracy: 0.3707\n",
      "Epoch 222/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1780 - accuracy: 0.3723\n",
      "Epoch 223/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1707 - accuracy: 0.3733\n",
      "Epoch 224/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1845 - accuracy: 0.3716\n",
      "Epoch 225/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1737 - accuracy: 0.3721\n",
      "Epoch 226/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1743 - accuracy: 0.3729\n",
      "Epoch 227/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1760 - accuracy: 0.3733\n",
      "Epoch 228/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1729 - accuracy: 0.3727\n",
      "Epoch 229/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1725 - accuracy: 0.3733\n",
      "Epoch 230/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1673 - accuracy: 0.3741\n",
      "Epoch 231/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1663 - accuracy: 0.3739\n",
      "Epoch 232/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1714 - accuracy: 0.3729\n",
      "Epoch 233/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1679 - accuracy: 0.3740\n",
      "Epoch 234/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1678 - accuracy: 0.3747\n",
      "Epoch 235/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1675 - accuracy: 0.3739\n",
      "Epoch 236/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1603 - accuracy: 0.3749\n",
      "Epoch 237/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1649 - accuracy: 0.3741\n",
      "Epoch 238/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.2090 - accuracy: 0.3699\n",
      "Epoch 239/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1753 - accuracy: 0.3734\n",
      "Epoch 240/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1636 - accuracy: 0.3735\n",
      "Epoch 241/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1637 - accuracy: 0.3740\n",
      "Epoch 242/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1633 - accuracy: 0.3742\n",
      "Epoch 243/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1635 - accuracy: 0.3740\n",
      "Epoch 244/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1585 - accuracy: 0.3747\n",
      "Epoch 245/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1584 - accuracy: 0.3746\n",
      "Epoch 246/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1594 - accuracy: 0.3754\n",
      "Epoch 247/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1568 - accuracy: 0.3751\n",
      "Epoch 248/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1616 - accuracy: 0.3740\n",
      "Epoch 249/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1541 - accuracy: 0.3752\n",
      "Epoch 250/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1565 - accuracy: 0.3750\n",
      "Epoch 251/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1491 - accuracy: 0.3769\n",
      "Epoch 252/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1558 - accuracy: 0.3744\n",
      "Epoch 253/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1546 - accuracy: 0.3752\n",
      "Epoch 254/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1635 - accuracy: 0.3740\n",
      "Epoch 255/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1545 - accuracy: 0.3747\n",
      "Epoch 256/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1566 - accuracy: 0.3762\n",
      "Epoch 257/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1579 - accuracy: 0.3754\n",
      "Epoch 258/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1550 - accuracy: 0.3760\n",
      "Epoch 259/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1555 - accuracy: 0.3757\n",
      "Epoch 260/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1504 - accuracy: 0.3765\n",
      "Epoch 261/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1493 - accuracy: 0.3773\n",
      "Epoch 262/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1546 - accuracy: 0.3758\n",
      "Epoch 263/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1524 - accuracy: 0.3759\n",
      "Epoch 264/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1518 - accuracy: 0.3757\n",
      "Epoch 265/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1548 - accuracy: 0.3760\n",
      "Epoch 266/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1488 - accuracy: 0.3760\n",
      "Epoch 267/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1537 - accuracy: 0.3753\n",
      "Epoch 268/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1575 - accuracy: 0.3749\n",
      "Epoch 269/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1399 - accuracy: 0.3782\n",
      "Epoch 270/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1424 - accuracy: 0.3767\n",
      "Epoch 271/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1569 - accuracy: 0.3747\n",
      "Epoch 272/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1412 - accuracy: 0.3775\n",
      "Epoch 273/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1433 - accuracy: 0.3763\n",
      "Epoch 274/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1503 - accuracy: 0.3763\n",
      "Epoch 275/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1594 - accuracy: 0.3750\n",
      "Epoch 276/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1381 - accuracy: 0.3767\n",
      "Epoch 277/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1482 - accuracy: 0.3771\n",
      "Epoch 278/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1416 - accuracy: 0.3769\n",
      "Epoch 279/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1420 - accuracy: 0.3765\n",
      "Epoch 280/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1758 - accuracy: 0.3709\n",
      "Epoch 281/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1387 - accuracy: 0.3774\n",
      "Epoch 282/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1414 - accuracy: 0.3781\n",
      "Epoch 283/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1437 - accuracy: 0.3767\n",
      "Epoch 284/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1400 - accuracy: 0.3770\n",
      "Epoch 285/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1418 - accuracy: 0.3775\n",
      "Epoch 286/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1539 - accuracy: 0.3754\n",
      "Epoch 287/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1306 - accuracy: 0.3788\n",
      "Epoch 288/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1328 - accuracy: 0.3786\n",
      "Epoch 289/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1438 - accuracy: 0.3764\n",
      "Epoch 290/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1452 - accuracy: 0.3764\n",
      "Epoch 291/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1403 - accuracy: 0.3764\n",
      "Epoch 292/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1390 - accuracy: 0.3774\n",
      "Epoch 293/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1365 - accuracy: 0.3770\n",
      "Epoch 294/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1482 - accuracy: 0.3771\n",
      "Epoch 295/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1397 - accuracy: 0.3769\n",
      "Epoch 296/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1386 - accuracy: 0.3775\n",
      "Epoch 297/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1406 - accuracy: 0.3778\n",
      "Epoch 298/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1440 - accuracy: 0.3765\n",
      "Epoch 299/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1461 - accuracy: 0.3762\n",
      "Epoch 300/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1369 - accuracy: 0.3780\n",
      "Epoch 301/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1345 - accuracy: 0.3782\n",
      "Epoch 302/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1362 - accuracy: 0.3784\n",
      "Epoch 303/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1331 - accuracy: 0.3782\n",
      "Epoch 304/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1343 - accuracy: 0.3781\n",
      "Epoch 305/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1326 - accuracy: 0.3791\n",
      "Epoch 306/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1351 - accuracy: 0.3773\n",
      "Epoch 307/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1321 - accuracy: 0.3790\n",
      "Epoch 308/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1377 - accuracy: 0.3778\n",
      "Epoch 309/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1355 - accuracy: 0.3792\n",
      "Epoch 310/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1338 - accuracy: 0.3783\n",
      "Epoch 311/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1291 - accuracy: 0.3779\n",
      "Epoch 312/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1349 - accuracy: 0.3779\n",
      "Epoch 313/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1335 - accuracy: 0.3782\n",
      "Epoch 314/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1349 - accuracy: 0.3782\n",
      "Epoch 315/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1420 - accuracy: 0.3766\n",
      "Epoch 316/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1421 - accuracy: 0.3774\n",
      "Epoch 317/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1307 - accuracy: 0.3788\n",
      "Epoch 318/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1488 - accuracy: 0.3771\n",
      "Epoch 319/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1307 - accuracy: 0.3787\n",
      "Epoch 320/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1305 - accuracy: 0.3788\n",
      "Epoch 321/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1283 - accuracy: 0.3788\n",
      "Epoch 322/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1286 - accuracy: 0.3789\n",
      "Epoch 323/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1280 - accuracy: 0.3786\n",
      "Epoch 324/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1201 - accuracy: 0.3805\n",
      "Epoch 325/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1284 - accuracy: 0.3794\n",
      "Epoch 326/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1223 - accuracy: 0.3798\n",
      "Epoch 327/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1278 - accuracy: 0.3788\n",
      "Epoch 328/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1267 - accuracy: 0.3785\n",
      "Epoch 329/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1321 - accuracy: 0.3771\n",
      "Epoch 330/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1331 - accuracy: 0.3773\n",
      "Epoch 331/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1270 - accuracy: 0.3791\n",
      "Epoch 332/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1241 - accuracy: 0.3800\n",
      "Epoch 333/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1309 - accuracy: 0.3786\n",
      "Epoch 334/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1253 - accuracy: 0.3796\n",
      "Epoch 335/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1340 - accuracy: 0.3780\n",
      "Epoch 336/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1370 - accuracy: 0.3770\n",
      "Epoch 337/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1348 - accuracy: 0.3776\n",
      "Epoch 338/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1229 - accuracy: 0.3794\n",
      "Epoch 339/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1286 - accuracy: 0.3791\n",
      "Epoch 340/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1228 - accuracy: 0.3792\n",
      "Epoch 341/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1282 - accuracy: 0.3780\n",
      "Epoch 342/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1422 - accuracy: 0.3755\n",
      "Epoch 343/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1237 - accuracy: 0.3790\n",
      "Epoch 344/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1240 - accuracy: 0.3785\n",
      "Epoch 345/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1271 - accuracy: 0.3789\n",
      "Epoch 346/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1197 - accuracy: 0.3800\n",
      "Epoch 347/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1267 - accuracy: 0.3785\n",
      "Epoch 348/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1259 - accuracy: 0.3793\n",
      "Epoch 349/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1141 - accuracy: 0.3796\n",
      "Epoch 350/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1392 - accuracy: 0.3768\n",
      "Epoch 351/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1206 - accuracy: 0.3792\n",
      "Epoch 352/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1204 - accuracy: 0.3796\n",
      "Epoch 353/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1222 - accuracy: 0.3790\n",
      "Epoch 354/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1277 - accuracy: 0.3785\n",
      "Epoch 355/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1230 - accuracy: 0.3781\n",
      "Epoch 356/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1218 - accuracy: 0.3791\n",
      "Epoch 357/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1265 - accuracy: 0.3782\n",
      "Epoch 358/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1298 - accuracy: 0.3783\n",
      "Epoch 359/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1235 - accuracy: 0.3791\n",
      "Epoch 360/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1201 - accuracy: 0.3791\n",
      "Epoch 361/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1245 - accuracy: 0.3784\n",
      "Epoch 362/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1192 - accuracy: 0.3804\n",
      "Epoch 363/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1204 - accuracy: 0.3790\n",
      "Epoch 364/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1201 - accuracy: 0.3793\n",
      "Epoch 365/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1191 - accuracy: 0.3803\n",
      "Epoch 366/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1206 - accuracy: 0.3784\n",
      "Epoch 367/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1207 - accuracy: 0.3794\n",
      "Epoch 368/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1119 - accuracy: 0.3802\n",
      "Epoch 369/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1175 - accuracy: 0.3798\n",
      "Epoch 370/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1135 - accuracy: 0.3806\n",
      "Epoch 371/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1225 - accuracy: 0.3787\n",
      "Epoch 372/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1195 - accuracy: 0.3786\n",
      "Epoch 373/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1205 - accuracy: 0.3788\n",
      "Epoch 374/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1294 - accuracy: 0.3782\n",
      "Epoch 375/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1237 - accuracy: 0.3791\n",
      "Epoch 376/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1331 - accuracy: 0.3772\n",
      "Epoch 377/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1221 - accuracy: 0.3792\n",
      "Epoch 378/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1277 - accuracy: 0.3789\n",
      "Epoch 379/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1270 - accuracy: 0.3785\n",
      "Epoch 380/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1273 - accuracy: 0.3776\n",
      "Epoch 381/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1348 - accuracy: 0.3781\n",
      "Epoch 382/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1271 - accuracy: 0.3796\n",
      "Epoch 383/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1205 - accuracy: 0.3794\n",
      "Epoch 384/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1289 - accuracy: 0.3785\n",
      "Epoch 385/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1205 - accuracy: 0.3794\n",
      "Epoch 386/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1364 - accuracy: 0.3780\n",
      "Epoch 387/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1238 - accuracy: 0.3789\n",
      "Epoch 388/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1266 - accuracy: 0.3785\n",
      "Epoch 389/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1208 - accuracy: 0.3796\n",
      "Epoch 390/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1163 - accuracy: 0.3811\n",
      "Epoch 391/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1247 - accuracy: 0.3793\n",
      "Epoch 392/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1187 - accuracy: 0.3800\n",
      "Epoch 393/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1227 - accuracy: 0.3792\n",
      "Epoch 394/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1207 - accuracy: 0.3793\n",
      "Epoch 395/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1184 - accuracy: 0.3795\n",
      "Epoch 396/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1189 - accuracy: 0.3799\n",
      "Epoch 397/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1156 - accuracy: 0.3793\n",
      "Epoch 398/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1229 - accuracy: 0.3790\n",
      "Epoch 399/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1197 - accuracy: 0.3795\n",
      "Epoch 400/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1138 - accuracy: 0.3802\n",
      "Epoch 401/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1151 - accuracy: 0.3798\n",
      "Epoch 402/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1294 - accuracy: 0.3779\n",
      "Epoch 403/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1151 - accuracy: 0.3798\n",
      "Epoch 404/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1151 - accuracy: 0.3799\n",
      "Epoch 405/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1157 - accuracy: 0.3800\n",
      "Epoch 406/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1218 - accuracy: 0.3789\n",
      "Epoch 407/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1216 - accuracy: 0.3788\n",
      "Epoch 408/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1209 - accuracy: 0.3799\n",
      "Epoch 409/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1208 - accuracy: 0.3787\n",
      "Epoch 410/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1195 - accuracy: 0.3803\n",
      "Epoch 411/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1246 - accuracy: 0.3795\n",
      "Epoch 412/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1216 - accuracy: 0.3793\n",
      "Epoch 413/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1207 - accuracy: 0.3787\n",
      "Epoch 414/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1199 - accuracy: 0.3791\n",
      "Epoch 415/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1187 - accuracy: 0.3791\n",
      "Epoch 416/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1189 - accuracy: 0.3787\n",
      "Epoch 417/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1081 - accuracy: 0.3814\n",
      "Epoch 418/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1062 - accuracy: 0.3808\n",
      "Epoch 419/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1096 - accuracy: 0.3811\n",
      "Epoch 420/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1205 - accuracy: 0.3790\n",
      "Epoch 421/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1224 - accuracy: 0.3781\n",
      "Epoch 422/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1175 - accuracy: 0.3800\n",
      "Epoch 423/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1145 - accuracy: 0.3795\n",
      "Epoch 424/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1125 - accuracy: 0.3798\n",
      "Epoch 425/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1147 - accuracy: 0.3793\n",
      "Epoch 426/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1173 - accuracy: 0.3794\n",
      "Epoch 427/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1117 - accuracy: 0.3809\n",
      "Epoch 428/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1223 - accuracy: 0.3793\n",
      "Epoch 429/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1190 - accuracy: 0.3794\n",
      "Epoch 430/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1247 - accuracy: 0.3791\n",
      "Epoch 431/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1195 - accuracy: 0.3796\n",
      "Epoch 432/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1210 - accuracy: 0.3795\n",
      "Epoch 433/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1149 - accuracy: 0.3785\n",
      "Epoch 434/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1190 - accuracy: 0.3786\n",
      "Epoch 435/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1253 - accuracy: 0.3783\n",
      "Epoch 436/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1240 - accuracy: 0.3784\n",
      "Epoch 437/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1302 - accuracy: 0.3783\n",
      "Epoch 438/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1135 - accuracy: 0.3791\n",
      "Epoch 439/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1145 - accuracy: 0.3794\n",
      "Epoch 440/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1187 - accuracy: 0.3792\n",
      "Epoch 441/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1222 - accuracy: 0.3793\n",
      "Epoch 442/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1105 - accuracy: 0.3799\n",
      "Epoch 443/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1137 - accuracy: 0.3804\n",
      "Epoch 444/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1119 - accuracy: 0.3809\n",
      "Epoch 445/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1220 - accuracy: 0.3798\n",
      "Epoch 446/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1170 - accuracy: 0.3810\n",
      "Epoch 447/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1137 - accuracy: 0.3814\n",
      "Epoch 448/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1150 - accuracy: 0.3801\n",
      "Epoch 449/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1182 - accuracy: 0.3794\n",
      "Epoch 450/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1160 - accuracy: 0.3787\n",
      "Epoch 451/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1187 - accuracy: 0.3792\n",
      "Epoch 452/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1223 - accuracy: 0.3789\n",
      "Epoch 453/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1225 - accuracy: 0.3789\n",
      "Epoch 454/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1113 - accuracy: 0.3805\n",
      "Epoch 455/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1127 - accuracy: 0.3808\n",
      "Epoch 456/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1193 - accuracy: 0.3791\n",
      "Epoch 457/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1080 - accuracy: 0.3800\n",
      "Epoch 458/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1217 - accuracy: 0.3787\n",
      "Epoch 459/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1131 - accuracy: 0.3804\n",
      "Epoch 460/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1169 - accuracy: 0.3796\n",
      "Epoch 461/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1149 - accuracy: 0.3801\n",
      "Epoch 462/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1214 - accuracy: 0.3794\n",
      "Epoch 463/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1064 - accuracy: 0.3810\n",
      "Epoch 464/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1143 - accuracy: 0.3795\n",
      "Epoch 465/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1195 - accuracy: 0.3796\n",
      "Epoch 466/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1097 - accuracy: 0.3813\n",
      "Epoch 467/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1156 - accuracy: 0.3798\n",
      "Epoch 468/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1118 - accuracy: 0.3797\n",
      "Epoch 469/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1126 - accuracy: 0.3806\n",
      "Epoch 470/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1085 - accuracy: 0.3811\n",
      "Epoch 471/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1199 - accuracy: 0.3792\n",
      "Epoch 472/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1127 - accuracy: 0.3805\n",
      "Epoch 473/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1208 - accuracy: 0.3788\n",
      "Epoch 474/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1163 - accuracy: 0.3801\n",
      "Epoch 475/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1247 - accuracy: 0.3794\n",
      "Epoch 476/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1235 - accuracy: 0.3796\n",
      "Epoch 477/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1161 - accuracy: 0.3798\n",
      "Epoch 478/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1241 - accuracy: 0.3796\n",
      "Epoch 479/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1172 - accuracy: 0.3797\n",
      "Epoch 480/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1150 - accuracy: 0.3799\n",
      "Epoch 481/500\n",
      "6130/6130 [==============================] - 67s 11ms/step - loss: 3.1097 - accuracy: 0.3810\n",
      "Epoch 482/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1191 - accuracy: 0.3800\n",
      "Epoch 483/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1165 - accuracy: 0.3799\n",
      "Epoch 484/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1203 - accuracy: 0.3794\n",
      "Epoch 485/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1149 - accuracy: 0.3794\n",
      "Epoch 486/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1199 - accuracy: 0.3796\n",
      "Epoch 487/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1133 - accuracy: 0.3796\n",
      "Epoch 488/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1173 - accuracy: 0.3799\n",
      "Epoch 489/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1155 - accuracy: 0.3802\n",
      "Epoch 490/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1136 - accuracy: 0.3803\n",
      "Epoch 491/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1161 - accuracy: 0.3798\n",
      "Epoch 492/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1276 - accuracy: 0.3790\n",
      "Epoch 493/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1161 - accuracy: 0.3799\n",
      "Epoch 494/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1256 - accuracy: 0.3783\n",
      "Epoch 495/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1133 - accuracy: 0.3804\n",
      "Epoch 496/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1127 - accuracy: 0.3798\n",
      "Epoch 497/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1225 - accuracy: 0.3788\n",
      "Epoch 498/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1110 - accuracy: 0.3799\n",
      "Epoch 499/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1161 - accuracy: 0.3789\n",
      "Epoch 500/500\n",
      "6130/6130 [==============================] - 66s 11ms/step - loss: 3.1135 - accuracy: 0.3799\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(xs, ys, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7-Y1eYFnPLwa",
    "outputId": "15832cd9-d6fc-4bd9-94ef-55d3c5275360"
   },
   "outputs": [],
   "source": [
    "model.save('ErpPresModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7N-WAI-NPReP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "wNRXeMAjPYgD",
    "outputId": "9216e973-78f3-47dc-9fb0-1fa61636d079"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gH_NOUUvevEV"
   },
   "source": [
    "## Text Generation\n",
    "\n",
    "I put the seed text as below. The model will generate 100 words after the seed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNFUnCEGPc3c"
   },
   "outputs": [],
   "source": [
    "seed_text = \"I have some proposals to the Congress\"\n",
    "next_words = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "T2DzxoitP8hq",
    "outputId": "195e9396-30d6-465d-e840-0126a06e39bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-3d7d119a8e44>:4: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "I have some proposals to the Congress of the first months of the past year have been accomplished to keep that matter up their abundance ending upon the war ii and hearts inflation continues to adjust peoples testifies nor paperwork proposals know moreover coverage to assures visible in our economic destabilizing coverage overregulated that skillfully assures we beef destabilizing coverage under coverage implies antitrust destabilizing fiscally testify to approve urban coverage coverage coverage and stymies consumption authorities dividend coverage appropriated coverage reaching else coverage propose proposals propose proposals stymies preference loans continued values meant that urgent coverage downward know coverage fostered that referred sdr disease coverage for\n"
     ]
    }
   ],
   "source": [
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAe6drImfHJU"
   },
   "source": [
    "Looks like English. Sounds like presidential?\n",
    "\n",
    "My ambition was to create some A. I. Presidents, like a Democrat President or a Republican President, and let them talk about health care, tax, trade, etc. However, the generated texts so far are not convincing. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "let_pres_speak_failed.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
