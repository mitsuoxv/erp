---
title: "Sentiment analysis of the economic report of the president"
author: "Mitsuo Shiota"
date: "2020/3/2"
output: 
  github_document:
    toc: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Updated: `r Sys.Date()`

## Motivation

After I have done [td-idf analysis](https://github.com/mitsuoxv/erp/blob/master/README.md), I would like to do sentiment analysis of the economic report of the president. Basically, I follow the codes in ["Text Mining Fedspeak" by Len Kiefer](http://lenkiefer.com/2018/07/28/text-mining-fedspeak/).

```{r libraries, include=FALSE}
library(tidyverse)
library(tidytext)
```

## Prepare cleaned-up texts and tidytext data

The files in `texts/presidents/`, such as `1947_pres.txt`, are the president parts which I have manually cleaned up. I will do sentiment analysis to the president part, not including the Council of Economic Advisers part.

I cleaned up by:

1. correcting word order, where digitization made mistakes in shaping lines;

1. correcting words, where optical recognition made mistakes due to dirt, or the author apparently misspelled;

1. making punctuation common over reports, following my memory of ["The Mac is Not a Typewriter" by Robin P. Williams](https://www.goodreads.com/book/show/41600.The_Mac_is_Not_a_Typewriter), which I must have, but could not find now. As an exception, using minus mark instead of hyphen; and

1. changing lines, where I encounter ";", ":", "â€”" or "," and when I feel changed lines are natural if they were in President Johnson's reports.

I tried to be consistent over reports of different authors. Honestly, I am not sure whether "text" has become more consistent or not, as a result.

```{r preparation, echo=FALSE}
files <- dir("texts/presidents/")

erp_text_raw <- vector("list", length = length(files))

for (i in seq_along(erp_text_raw)) {
  erp_text_raw[[i]] <- 
    tibble(
      text = readLines(str_c("texts/presidents/", files[i])),
      year = as.integer(str_sub(files[i], 1L, 4L))
    )
}

erp_text_raw_df <- bind_rows(erp_text_raw)

erp_text_df <- erp_text_raw_df %>% 
  group_by(year) %>% 
  mutate(line = row_number()) %>% 
  ungroup()

# remove bullet only lines
erp_text_df <- erp_text_df %>% 
  filter(!str_detect(text, "^.$")) %>% 
  filter(!str_detect(text, "^\\(.\\)$")) %>% 
  filter(!str_detect(text, "^.\\.$"))

erp_text <- erp_text_df %>% 
  unnest_tokens(word, text)
```

To refresh memory, I show some variables below.

erp_text_raw_df is a data frame consisted of 3 colums: "year", "line" and "text". "text" is basically a sentence, but in some cases, it is a clause.

```{r erp_text_df}
erp_text_df
```

erp_text is a data frame after "text" was tokenized into "word". It has 3 colums: "year", "line", "word".

```{r erp_text}
erp_text
```

## Whose talks are more verbose?

Old Democrats tend to write more lines and words in a report.

```{r n_lines_per_report, echo=FALSE}
year_pres <- tibble(
  year = 1947:2021,
  author = c(
    rep("Truman", 7),
    rep("Eisenhower", 8),
    rep("Kennedy", 2),
    rep("Johnson", 6),
    rep("Nixon", 5),
    rep("Ford", 3),
    rep("Carter", 4),
    rep("Reagan", 8),
    rep("Bush, H.W.", 4),
    rep("Clinton", 8),
    rep("Bush, W.", 8),
    rep("Obama", 8),
    rep("Trump", 4)
  )
)

party_pres <- tibble(
  author = unique(year_pres$author),
  party = c("D", "R", "D", "D", "R", "R", "D", "R", "R", "D", "R", "D", "R")
)

pres_lines <- erp_text_df %>% 
  group_by(year) %>% 
  count(year) %>% 
  left_join(year_pres, by = "year") %>% 
  left_join(party_pres, by = "author")

pres_lines$author <- factor(pres_lines$author, levels = unique(year_pres$author))

pres_lines %>% 
  ggplot(aes(author, n)) +
  geom_boxplot(aes(color = party)) +
  coord_flip() +
  labs(y = "number of lines per report") +
  scale_color_manual(values = c("blue", "red"))
```

```{r n_words_per_report, echo=FALSE}
pres_words <- erp_text %>% 
  group_by(year) %>% 
  count(year) %>% 
  left_join(year_pres, by = "year") %>% 
  left_join(party_pres, by = "author")

pres_words$author <- factor(pres_words$author, levels = unique(year_pres$author))

pres_words %>% 
  ggplot(aes(author, n)) +
  geom_boxplot(aes(color = party)) +
  coord_flip() +
  labs(y = "number of words per report") +
  scale_color_manual(values = c("blue", "red"))
```

The number of words per line does not vary much among the Presidents, partly because I have somewhat arbitrarily changed lines by trying to be consistent over different authors.

```{r n_words_per_line, echo=FALSE}
words_per_line <- erp_text %>% 
  group_by(year, line) %>% 
  count(line) %>% 
  left_join(year_pres, by = "year") %>% 
  left_join(party_pres, by = "author")

words_per_line$author <- factor(words_per_line$author, levels = unique(year_pres$author))

words_per_line %>% 
  ggplot(aes(author, n)) +
  geom_boxplot(aes(color = party)) +
  coord_flip() +
  labs(y = "number of words per line") +
  scale_color_manual(values = c("blue", "red"))
```

Distribution concentrates from 5 to 34 words per line.

```{r words_per_line2, echo=FALSE}
erp_text %>% 
  group_by(year, line) %>% 
  count(line) %>% 
  ggplot(aes(n)) +
  geom_histogram(binwidth = 5, boundary = 0) +
  labs(x = "number of words per line")
```

President Johnson wrote the most lines in his 8 reports. The second is Truman in his 7 reports, and the third is Carter in his 4 reports. My attempt to  [let the presidents speak](https://github.com/mitsuoxv/erp/blob/master/let_pres_speak.ipynb) is biased to old Democrats.

```{r n_lines_total, echo=FALSE}
pres_lines_total <- erp_text_df %>% 
  left_join(year_pres, by = "year") %>% 
  group_by(author) %>% 
  count(author) %>% 
  left_join(party_pres, by = "author")

pres_lines_total$author <- factor(pres_lines_total$author, levels = unique(year_pres$author))

pres_lines_total %>% 
  ggplot(aes(author, n)) +
  geom_col(aes(fill = party)) +
  coord_flip() +
  labs(y = "number of lines in total") +
  scale_fill_manual(values = c("blue", "red"))
```

## Check sentimental words by bigram

I use "bing" to get sentimental words, which show either "positive" or "negative". Among the sentimental words, "benefits" appear most in the president part of the economic report of the president.

```{r sentiment_word_rank}
sentiment_word_rank <- erp_text %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = TRUE) %>% 
  mutate(rank = rank(desc(n), ties.method = "random"))

sentiment_word_rank
```

Some of sentimental words in "bing" are just technical terms in economic reports. "debt" is such an example.

"gross" is likely to be a part of technical terms, like "gross domestic product". Thus Len Kiefer suspects in ["Text Mining Fedspeak"](http://lenkiefer.com/2018/07/28/text-mining-fedspeak/).

I suspect "benefits" is another example, as it appears in the technical terms like "social benefits".

```{r gross}
sentiment_word_rank %>% 
  filter(word == "gross")
```

```{r benefits}
sentiment_word_rank %>% 
  filter(word == "benefits")
```

Let's check Len Kiefer's and my suspicion. I go back to erp_text_df, and tokenize "text" not by a single word into "word", but by 2 consecutive words into "bigram" this time. Then I get erp_bigrams, which is a data frame of 4 colums: "year", "page", "line" and "bigram".

```{r bigram}
erp_bigrams <-   
  erp_text_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  drop_na(bigram)

erp_bigrams
```

Most frequently used bigrams are uninteresting, as they include stop words.

```{r count_bigram}
erp_bigrams %>%
  count(bigram, sort = TRUE)
```

So I separate "bigram" into "word1" and "word2", and filter so that either "word1" or "word2" is not a stop word. After filtering, most frequently used bigrams are "labor force", etc. They show these reports are indeed about the economy.

```{r count_bigram_wo_stop_words}
bigrams_separated <- erp_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)
```

What word follows "gross" most frequently? As Len Kiefer suspects, "gross" is a part of technical terms, and should not be included in the sentimental words.

```{r check_gross}
bigrams_filtered %>%
  filter(word1 == "gross") %>%
  count(word2, sort = TRUE)
```

What word precedes "benefits" most frequently? My suspicion is confirmed.

```{r check_benefits}
bigrams_filtered %>%
  filter(word2 == "benefits") %>%
  count(word1, sort = TRUE)
```

So I add "benefits" to the stop words prepared by Len Kiefer in ["Text Mining Fedspeak"](http://lenkiefer.com/2018/07/28/text-mining-fedspeak/).

```{r stop_words}
custom_stop_words2 <- 
  bind_rows(tibble(word = c("benefits",
                            "debt",
                            "gross",
                            "crude",
                            "well",
                            "maturity",
                            "work",
                            "marginally",
                            "leverage"),
                   lexicon = c("custom")),
            stop_words)
```

## Sentiment analysis

I count both positive and negative words, and take the difference as "sentiment". I draw "sentiment" by year. Looks like the presidents before 1970 were optimistic. But wait. 

```{r not_standardaized_sentiment}
erp_text %>%
  anti_join(custom_stop_words2, by = "word") %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(year, sentiment) %>%
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  ggplot(aes(year, sentiment)) +
  geom_line()
```

As I count the number of words, "sentiment" gets bigger either in positive or negative direction, as the number of words increase. So I decide to standardize by the number of words excluding stop words. First I count the number of words excluding stop words by year.

```{r count_words}
erp_words <-
  erp_text %>%
  anti_join(custom_stop_words2, by = "word") %>% 
  group_by(year) %>% 
  count()
```

I prepare to draw shaded areas of recession. I owe this technique also to Len Kiefer, as I refer to his ["Plotting U.S. Macroeconomic Trends with FRED and R"](http://lenkiefer.com/2017/12/11/plotting-u-s-macroeconomic-trends-with-fred-and-r/).

```{r recession_df}
recessions_df = tribble(
  ~Peak, ~Trough,
  #----------|----------
  "1948-11-01", "1949-10-01",
  "1953-07-01", "1954-05-01",
  "1957-08-01", "1958-04-01",
  "1960-04-01", "1961-02-01",
  "1969-12-01", "1970-11-01",
  "1973-11-01", "1975-03-01",
  "1980-01-01", "1980-07-01",
  "1981-07-01", "1982-11-01",
  "1990-07-01", "1991-03-01",
  "2001-03-01", "2001-11-01",
  "2007-12-01", "2009-06-01",
  "2020-02-01", "2021-02-01"
)

recessions_df <- recessions_df %>% 
  mutate(
    Peak = as.Date(Peak),
    Trough = as.Date(Trough)
  )

```

As the reports were mostly published in January or February of each year, I put the points in February in each year. The shaded areas are recessions. The sentiments tend to fall after the recession, but not always.

```{r standardaized_sentiment}
erp_text %>%
  anti_join(custom_stop_words2, by = "word") %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(year, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  left_join(erp_words, by = "year") %>% 
  mutate(sentiment = (positive - negative) / n) %>% 
  mutate(publish = as.Date(str_c(year, "-02-01"))) %>% 
  ggplot(aes(publish, sentiment)) +
  geom_hline(yintercept = 0, color = "white", size = 2) +
  geom_rect(data = recessions_df, inherit.aes = FALSE,
            aes(xmin = Peak, xmax = Trough, ymin = -Inf, ymax = +Inf),
            fill='darkgray', alpha=0.5) +
  geom_line() +
  xlab(NULL)
```

EOL
